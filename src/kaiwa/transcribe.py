"""kaiwa â€” æ–‡å­—èµ·ã“ã—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

WhisperX ã‚’ä½¿ç”¨ã—ãŸéŸ³å£°æ–‡å­—èµ·ã“ã— + ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆå‡¦ç†ã€‚
"""

from __future__ import annotations

# ============================================================
# torch.load ãƒ‘ãƒƒãƒï¼ˆPyTorch 2.8 + pyannote äº’æ›æ€§ã®ãŸã‚ï¼‰
# å¿…ãšä»–ã® import ã‚ˆã‚Šå‰ã«é©ç”¨ã™ã‚‹ã“ã¨
# ============================================================
import torch
import warnings

_original_torch_load = torch.load


def _patched_torch_load(*args, **kwargs):
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è­¦å‘Šï¼ˆåˆå›ã®ã¿ï¼‰
    warnings.warn(
        "torch.load ã§ weights_only=False ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚"
        "ä¿¡é ¼ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã‚½ãƒ¼ã‚¹ï¼ˆHuggingFaceå…¬å¼ï¼‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚",
        category=RuntimeWarning,
        stacklevel=2
    )
    kwargs["weights_only"] = False
    return _original_torch_load(*args, **kwargs)


torch.load = _patched_torch_load

# ============================================================
# é€šå¸¸ã® importï¼ˆtorch.load ãƒ‘ãƒƒãƒã®å¾Œã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
# ============================================================
import logging  # noqa: E402
from pathlib import Path  # noqa: E402
from typing import Any  # noqa: E402

import whisperx  # noqa: E402

from kaiwa.utils import _save_intermediate  # noqa: E402

logger = logging.getLogger("kaiwa")


def transcribe(
    audio_path: Path,
    config: dict[str, Any],
    work_dir: Path | None = None,
) -> tuple[Any, dict[str, Any]]:
    """éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ WhisperX ã§æ–‡å­—èµ·ã“ã— + ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã™ã‚‹ã€‚

    Parameters
    ----------
    audio_path : Path
        å…¥åŠ›éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
    config : dict
        è¨­å®šè¾æ›¸ï¼ˆwhisper ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ï¼‰ã€‚
    work_dir : Path | None
        ä¸­é–“æˆæœç‰©ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚None ãªã‚‰ä¿å­˜ã—ãªã„ã€‚

    Returns
    -------
    tuple[Any, dict]
        (audio ãƒ‡ãƒ¼ã‚¿, ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆæ¸ˆã¿ result è¾æ›¸)ã€‚
    """
    whisper_cfg = config.get("whisper", {})
    device = whisper_cfg.get("device", "cpu")
    compute_type = whisper_cfg.get("compute_type", "float32")
    model_name = whisper_cfg.get("model", "large-v3-turbo")
    language = whisper_cfg.get("language", "ja")
    batch_size = whisper_cfg.get("batch_size", 8)

    if work_dir:
        work_dir.mkdir(parents=True, exist_ok=True)

    use_native_timestamps = whisper_cfg.get("use_native_word_timestamps", True)

    # whisperx.load_audio ã¯ diarize.py ã§ã‚‚ä½¿ã†ã®ã§å¸¸ã«å®Ÿè¡Œ
    audio = whisperx.load_audio(str(audio_path))

    if use_native_timestamps:
        # ----- faster-whisper ç›´æ¥ãƒ¢ãƒ¼ãƒ‰ï¼ˆword_timestamps å¯¾å¿œï¼‰ -----
        result = _transcribe_with_native_timestamps(
            audio_path, audio, model_name, device, compute_type, language,
        )
    else:
        # ----- WhisperX ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰ + wav2vec2 ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ -----
        result = _transcribe_with_whisperx(
            audio, audio_path, model_name, device, compute_type, language, batch_size,
        )

    logger.info("  âœ… æ–‡å­—èµ·ã“ã—å®Œäº†: %d ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", len(result["segments"]))

    # ä¸­é–“æˆæœç‰©ã‚’ä¿å­˜
    if work_dir:
        _save_intermediate(work_dir / "01_transcribe.json", result)

    return audio, result


def _transcribe_with_native_timestamps(
    audio_path: Path,
    audio: Any,
    model_name: str,
    device: str,
    compute_type: str,
    language: str,
) -> dict[str, Any]:
    """faster-whisper ã‚’ç›´æ¥ä½¿ã„ã€cross-attention ãƒ™ãƒ¼ã‚¹ã® word_timestamps ã‚’å–å¾—ã™ã‚‹ã€‚

    WhisperX ã®ãƒãƒƒãƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ word_timestamps ã«å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€
    faster-whisper ã® transcribe() ã‚’ç›´æ¥å‘¼ã³å‡ºã™ã€‚
    """
    import faster_whisper

    logger.info(
        "ğŸ“ æ–‡å­—èµ·ã“ã—é–‹å§‹ â€” native word_timestamps (model=%s, device=%s)",
        model_name, device,
    )

    model = faster_whisper.WhisperModel(
        model_name, device=device, compute_type=compute_type,
    )

    segments_gen, info = model.transcribe(
        str(audio_path),
        language=language,
        word_timestamps=True,
        vad_filter=True,  # VAD ã§ãƒã‚¤ã‚ºåŒºé–“ã‚’ã‚¹ã‚­ãƒƒãƒ—
    )

    segments: list[dict[str, Any]] = []
    for seg in segments_gen:
        words = []
        if seg.words:
            for w in seg.words:
                words.append({
                    "word": w.word,
                    "start": w.start,
                    "end": w.end,
                    "score": w.probability,
                })

        segments.append({
            "start": seg.start,
            "end": seg.end,
            "text": seg.text.strip(),
            "words": words,
        })

    logger.info("  â±ï¸  ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆä¸è¦ï¼ˆnative word_timestamps ä½¿ç”¨ï¼‰")

    return {"segments": segments, "language": info.language}


def _transcribe_with_whisperx(
    audio: Any,
    audio_path: Path,
    model_name: str,
    device: str,
    compute_type: str,
    language: str,
    batch_size: int,
) -> dict[str, Any]:
    """WhisperX ãƒãƒƒãƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + wav2vec2 ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆï¼ˆå¾“æ¥æ–¹å¼ï¼‰ã€‚"""
    logger.info(
        "ğŸ“ æ–‡å­—èµ·ã“ã—é–‹å§‹ â€” WhisperX batch + wav2vec2 align (model=%s, device=%s)",
        model_name, device,
    )

    model = whisperx.load_model(
        model_name,
        device=device,
        compute_type=compute_type,
        language=language,
    )

    result = model.transcribe(audio, batch_size=batch_size, language=language)

    logger.info("â±ï¸  ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆé–‹å§‹ï¼ˆwav2vec2ï¼‰...")

    align_model, align_metadata = whisperx.load_align_model(
        language_code=language, device=device,
    )
    result = whisperx.align(
        result["segments"],
        align_model,
        align_metadata,
        audio,
        device=device,
        return_char_alignments=False,
    )

    logger.info("  âœ… ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆå®Œäº†")

    return result
